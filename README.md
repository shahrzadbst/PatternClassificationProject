# PatternClassificationProject
Designing &amp; evaluating a support vector classifier for Drug-Target Interaction (DTI) Prediction. Steps: experiment design, blind test. Precision used as performance criteria at 50% recall. Trained linear SVM had poor precision; adopted meta-learning. Ensemble method (voting classifier) combines SVM with 4 classifiers via soft voting.


This Project presents the procedure of designing and evaluating a support vector classifier for Drug-Target Interaction (DTI) Prediction. The involved steps from experiment design to blind test for the evaluation of generalization capability of the classifier are described in detail throughout the report. The precision metric is used as the performance evaluation criteria at 50% recall. The trained support vector classifier with linear kernel resulted in poor precision estimates. For this very reason, the meta-learning approach is adopted to enhance performance. Among different available options, the ensemble method with voting classifier is selected in which the SVM method is combined with four different classifiers by soft voting. The resulting classifier shows improvement in precision metrics. Although the overall precision in the voting classifier is still far from good, it performed significantly well on the blind test in the sense of the validity of the predictions.

Drug repurposing is an efficient way of reusing existing drugs for curing new diseases, which benefit from reduced costs in terms of time, effort, and financial resources, as compared to drug discovery. Pattern classification can play a pivotal role in drug repurposing by predicting drug-target interactions (DTI) based on the existing DTI data from previous studies and providing the researchers with candidates with a high potential of interaction to verify in the lab.

A meta-learning approach is an applicable classification method to the DTI prediction problem. Several component classifiers are created at the first level to produce initial interaction predictions in this method. These initial predictions are then fed to a second-level classifier to generate the final class labels with improved accuracy. This work, however, is focused on the application of SVMs for the second-level classifier only. The input features to this level are initial predictions of 24 component classifiers, 14 different predicted variables each, resulting in a total number of 336 features. The objective is to train an SVM to classify the DTIs as positive or negative using these features and provided training data.

The SVM method leverages the idea of mapping the data to a higher-dimensional space where it could be linearly separated into two or more categories by one or more hyperplanes. The SVM method uses an optimization technique to determine the hyperplane with the largest margin from the training data in the new feature space. This maximum margin criterion ensures the classifier's generalization capability that provides acceptable accuracy (misclassification rate) when applied to future data. However, based on the mentioned criterion alone, the resulting classifier might be overfitted to the training data set. The idea of soft margin in SVM solves this problem by enforcing a trade-off between misclassification and generalization objectives of the classifier so that the advantage of good generalization performance is preserved while misclassification error remains at an acceptable level.

For the case of DTI prediction, both of the linear kernels available in Scikit learn implementations were tested together with three other nonlinear kernels of the SVC module (poly, rbf, and sigmoid). Different sets of hyperparameters were tested in each experiment, including hardness, gamma, and degree (for polynomial kernel only). Still, the only case that converged to a solution was LinearSVC(). Therefore, the presented results for SVM in this report are limited to the linear kernel implemented by LinearSVC(), and the only hyperparameter to tune is C (hardness). For fine-tuning C, first, the upper and lower bounds are found by trial and error and in such a way that the training algorithm could converge to a solution in a reasonable number of iterations. Then the exhaustive grid search method (sklearn.model_selection.GridSearchCV) is applied to find the best parameter in the obtained range for C, resulting in the best value of C = 0.3. The maximum iteration in all experiments was set to 10000.

The performed preprocessing in this problem includes outlier detection, feature selection, and normalization. Considering the computational complexity of different outlier detection algorithms available in Scikit learn, the Local Outlier Factor (LOF) and the simpler interquartile methods were selected as initial candidates.

Performing a visual inspection to detect anomalous data was indecisive for this problem due to features with irregular class conditional distributions, as demonstrated in Figure 5-1. We then turned to the computational approach, starting with the LOF method. In a trial and error-based attempt for tuning the hyperparameters of the LOF algorithm, it became evident that the number of detected outliers varies extensively with different numbers of neighbors considered and the levels of contamination selected. Because no prior information could be found regarding the contamination level, the alternative approach, i.e., the interquartile method, was instead used selectively. That is, only features with unimodal class conditional distributions, like that of the G10_std_Target Sequence_dist feature in Figure 5-2, were considered. As a result, 2574 samples from training-validation data were marked as outliers.


![0D13FA26-D361-4757-A19A-0D5B782ABEA6](https://github.com/shahrzadbst/PatternClassificationProject/assets/92950700/156fc9cd-a85a-42ce-a696-4e4e685dc57f)


20-fold cross-validation is used, and the data is divided into train-validation and a hold-out independent test set. These data sets should be created from the original dataset using stratified sampling with random shuffling to account for class imbalance.
• The preprocessing stage mentioned earlier with more details, is performed on the train-validation data.
• The SVM classifier is trained and validated by a 20-fold CV for each set of hyperparameters in the exhaustive grid search. At the same time, performance metrics are computed for each classifier.
• The classifiers are tested on our hold-out independent dataset for generalization, and the best set of hyperparameters is selected.
• A new classifier is trained using all the available data and the best hyperparameters resulting from the exhaustive grid search and generalization test.
• Finally, the blind test data is processed applying the same preprocessing approach used in training (i.e., the same standardization parameters and selecting the same features). This standardized data is fed to the trained classifier, making blind score predictions.
The same procedure is applied for the ensemble classifier (i.e., the voting classifier) with slight modifications for adjusting the hyperparameters of the component classifiers other than SVM.

The idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote, or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be helpful for a set of equally well-performing models to balance out their individual weaknesses. Five different classifiers (LinearSVC, KNN, Decision Tree, GaussianNB, and LogisticRegression) were combined by soft voting and trained using stratified 20-fold cross-validation. For the LinearSVC component classifier, optimal hyperparameters were taken from the exhaustive grid search. The hyperparameters of the other four classifiers were tuned by trial and error.
